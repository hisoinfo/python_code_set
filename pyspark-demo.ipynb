{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark demo\n",
    "\n",
    "* 实例化spark并输出其版本号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc.stop()\n",
    "# 实例化spark\n",
    "appName = 'pysparkdemo'\n",
    "conf = SparkConf().setAppName(appName).setMaster('local[2]')\n",
    "# sc.stop()\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map \n",
    " * Return a new RDD by applying a function to each element of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, 2, 3]\n",
      "y: [1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print('x: {}'.format(x.collect()))\n",
    "print('y: {}'.format(y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions(f, preservesPartitioning=False)\n",
    "> Return a new RDD by applying a function to each partition of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[1], [2, 3]]\n",
      "y: [[1], [5]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "    \n",
    "y = x.mapPartitions(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print('x: {}'.format(x.glom().collect())) \n",
    "print('y: {}'.format(y.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex\n",
    " * Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3]]\n",
      "[[(0, 1)], [(1, 2)], [(2, 3)]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "x = sc.parallelize([1,2,3], 3)\n",
    "\n",
    "def f(partitionIndex, iterator): \n",
    "    yield (partitionIndex, sum(iterator))\n",
    "    \n",
    "y = x.mapPartitionsWithIndex(f)\n",
    " \n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())  \n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getNumPartitions \n",
    " * Returns the number of partitions in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "x = sc.parallelize([1,2,3,4,5], 2)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(f)\n",
    "* Return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct(numPartitions=None)\n",
    "* Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['B', 'A']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = x.distinct()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(withReplacement, fraction, seed=None)\n",
    "* Return a sampled subset of this RDD.\n",
    "\n",
    "> Parameters:\t\n",
    "* withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "* fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "* seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [2, 3, 6]\n",
      "sample:1 y = [0, 2, 3, 4, 5, 6]\n",
      "sample:2 y = [0, 3, 5]\n",
      "sample:3 y = [0, 3, 5, 6]\n",
      "sample:4 y = [0, 1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "x = sc.parallelize(range(7))\n",
    "# call 'sample' 5 times\n",
    "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)] \n",
    "\n",
    "print('x = {}'.format(str(x.collect())))\n",
    "\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample(withReplacement, num, seed=None)\n",
    "* Return a fixed-size sampled subset of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [1, 4, 5]\n",
      "sample:1 y = [3, 5, 1]\n",
      "sample:2 y = [2, 5, 1]\n",
      "sample:3 y = [3, 0, 5]\n",
      "sample:4 y = [2, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "# takeSample\n",
    "x = sc.parallelize(range(7))\n",
    "# call 'sample' 5 times\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  \n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histogram(buckets)\n",
    "* Compute a histogram using the provided buckets. The buckets are all open to the right except for the last which is closed. e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50], which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1 and 50 we would have a histogram of 1,0,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 25, 50], [25, 26])\n",
      "([0, 5, 25, 50], [5, 20, 26])\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize(range(51))\n",
    "\n",
    "# buckets 是数字时\n",
    "y = x.histogram(2)\n",
    "print(y)\n",
    "'''\n",
    "([0, 25, 50], [25, 26])\n",
    "参数buckets是2，输出两部分，[0,25,50]是桶，[25,26]是各个桶分布内的频数\n",
    "'''\n",
    "# buckets是列表时\n",
    "yl = x.histogram([0, 5, 25, 50])\n",
    "print(yl)\n",
    "'''\n",
    "([0, 5, 25, 50], [5, 20, 26])\n",
    "参数buckets是[0,5,25,50]，输出两部分，[0,5,25,50]是桶，[5,20,26]是各个桶分布内的频数\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"test.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "'''\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
